{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 79s 7us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Zver\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Zver\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Zver\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Zver\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Zver\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 900)               706500    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                9010      \n",
      "=================================================================\n",
      "Total params: 715,510\n",
      "Trainable params: 715,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(900, input_dim=784, activation=\"relu\", kernel_initializer=\"normal\"))\n",
    "#model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(10, activation=\"softmax\", kernel_initializer=\"normal\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/15\n",
      " - 22s - loss: 0.4450 - acc: 0.8638 - val_loss: 0.1466 - val_acc: 0.9617\n",
      "Epoch 2/15\n",
      " - 22s - loss: 0.2396 - acc: 0.9283 - val_loss: 0.1051 - val_acc: 0.9698\n",
      "Epoch 3/15\n",
      " - 21s - loss: 0.1949 - acc: 0.9412 - val_loss: 0.0895 - val_acc: 0.9755\n",
      "Epoch 4/15\n",
      " - 22s - loss: 0.1716 - acc: 0.9488 - val_loss: 0.0796 - val_acc: 0.9760\n",
      "Epoch 5/15\n",
      " - 22s - loss: 0.1558 - acc: 0.9526 - val_loss: 0.0729 - val_acc: 0.9782\n",
      "Epoch 6/15\n",
      " - 22s - loss: 0.1446 - acc: 0.9560 - val_loss: 0.0711 - val_acc: 0.9788\n",
      "Epoch 7/15\n",
      " - 22s - loss: 0.1375 - acc: 0.9572 - val_loss: 0.0701 - val_acc: 0.9797\n",
      "Epoch 8/15\n",
      " - 22s - loss: 0.1297 - acc: 0.9601 - val_loss: 0.0671 - val_acc: 0.9823\n",
      "Epoch 9/15\n",
      " - 22s - loss: 0.1237 - acc: 0.9620 - val_loss: 0.0625 - val_acc: 0.9817\n",
      "Epoch 10/15\n",
      " - 22s - loss: 0.1199 - acc: 0.9631 - val_loss: 0.0610 - val_acc: 0.9830\n",
      "Epoch 11/15\n",
      " - 22s - loss: 0.1179 - acc: 0.9640 - val_loss: 0.0612 - val_acc: 0.9833\n",
      "Epoch 12/15\n",
      " - 22s - loss: 0.1094 - acc: 0.9666 - val_loss: 0.0591 - val_acc: 0.9825\n",
      "Epoch 13/15\n",
      " - 22s - loss: 0.1071 - acc: 0.9663 - val_loss: 0.0623 - val_acc: 0.9825\n",
      "Epoch 14/15\n",
      " - 22s - loss: 0.1043 - acc: 0.9685 - val_loss: 0.0604 - val_acc: 0.9835\n",
      "Epoch 15/15\n",
      " - 22s - loss: 0.1003 - acc: 0.9679 - val_loss: 0.0589 - val_acc: 0.9840\n",
      "Accuracy on test set: 98.00999999999999\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "    batch_size=100, epochs=15, validation_split=0.1, verbose=2) \n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy on test set: {}\".format(scores[1] * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize training history\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/3\n",
      " - 22s - loss: 0.0896 - acc: 0.9721 - val_loss: 0.0590 - val_acc: 0.9842\n",
      "Epoch 2/3\n",
      " - 22s - loss: 0.0876 - acc: 0.9731 - val_loss: 0.0562 - val_acc: 0.9853\n",
      "Epoch 3/3\n",
      " - 22s - loss: 0.0882 - acc: 0.9730 - val_loss: 0.0583 - val_acc: 0.9837\n",
      "Accuracy on test set: 98.11\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "    batch_size=100, epochs=3, validation_split=0.1, verbose=2) \n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy on test set: {}\".format(scores[1] * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.2353406e-13, 1.7383503e-11, 1.5026981e-12, 8.4333188e-08,\n",
       "       1.1657320e-18, 2.2154897e-14, 8.7543327e-21, 9.9999988e-01,\n",
       "       1.5147007e-12, 1.6831606e-08], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_prime(x):\n",
    "    return 1.0 - np.square(np.tanh(x))\n",
    "\n",
    "def sigmoid(x, d=None):\n",
    "    if not d:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    else:\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# alternative activation function\n",
    "def ReLU(x, d=None):\n",
    "    if not d:\n",
    "        return np.maximum(0.0, x)\n",
    "    else:\n",
    "        return 1. * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet:\n",
    "    def __init__(self, activation1=ReLU, activation2=sigmoid):\n",
    "\n",
    "        self.activation1 = activation1\n",
    "        self.activation2 = activation2\n",
    "        \n",
    "        self.z2, self.a2, self.z3, self.a3 = (None,) * 4\n",
    "        self.delta2, self.delta3 = (None,) * 2\n",
    "        self.djdw1, self.djdw2 = (None,) * 2\n",
    "        self.gradient, self.numericalGradient = (None,) * 2\n",
    "        \n",
    "        w1 = np.random.rand(784,800) \n",
    "        self.w1 = np.vstack((w1, np.random.rand(1,800))) \n",
    "    \n",
    "        w2 = np.random.rand(800,10)\n",
    "        self.w2 =  np.vstack((w2, np.random.rand(1,10)))\n",
    "        \n",
    "                  \n",
    "    def forward(self, X):\n",
    "        self.m = float((X.shape[0]))\n",
    "\n",
    "        ba = np.ones((X.shape[0], 1))\n",
    "        self.x = np.concatenate((X, ba), axis=1)\n",
    "                       \n",
    "        self.z2 = np.dot(self.x, self.w1)\n",
    "        self.a2 = self.activation1(self.z2) \n",
    "\n",
    "        ba2 = np.ones((self.x.shape[0], 1))\n",
    "        self.a2 = np.concatenate((self.a2, ba2), axis=1)\n",
    "\n",
    "        self.z3 = np.dot(self.a2, self.w2)\n",
    "        self.a3 = self.activation2(self.z3)  \n",
    "        \n",
    "        \n",
    "    def backward(self, Y):\n",
    "        self.y = Y.copy() \n",
    "\n",
    "        self.delta3 = np.multiply(-(self.y - self.a3), \\\n",
    "                                  self.activation2(self.z3, 1))\n",
    "        \n",
    "        self.djdw2 = (self.a2.T.dot(self.delta3)) / \\\n",
    "                            self.m + self.Lambda *  self.w2\n",
    "                                                        #L1 np.sign(self.w2)\n",
    "\n",
    "        self.delta2 = np.multiply(self.delta3.dot(self.w2.T), \\\n",
    "                                  self.activation1(np.concatenate((self.z2, np.ones((self.z2.shape[0], 1))), axis=1), 1))\n",
    "        self.djdw1 = (self.x.T.dot(np.delete(self.delta2, 800, axis=1))) / self.m + self.Lambda *  self.w1\n",
    "                                                                                                    #L1 np.sign(self.w1)\n",
    "        \n",
    "    def update_gradient(self, lrate=100):\n",
    "        self.w1 -= lrate * self.djdw1\n",
    "        self.w2 -= lrate * self.djdw2\n",
    "\n",
    "        \n",
    "    def summary(self, step):\n",
    "        print( \"Iteration: %d, Loss %f\" % (step, self.cost_function()))\n",
    "        print(\"RMSE: \" + str(np.sqrt(np.mean(np.square(self.a3 - self.y)))))\n",
    "        print(\"MAE: \" + str(np.sum(np.absolute(self.a3 - self.y)) / self.m))    \n",
    "\n",
    "     \n",
    "    def cost_function(self):\n",
    "        return 0.5 * np.sum(np.square((self.y-self.a3))) / self.m + (self.Lambda/2) * (np.sum(np.square(self.w1)) + np.sum(np.square(self.w2)))\n",
    "                                                        # L1 self.Lambda * (np.sum(np.abs(self.w1)) + np.sum(np.abs(self.w1)))\n",
    "        \n",
    "        \n",
    "    def train(self, X, Y, iters=1000, lrate=100, Lambda=0.001):\n",
    "        self.Lambda = Lambda\n",
    "        for step in range(iters):\n",
    "            self.forward(X)\n",
    "            self.backward(Y)\n",
    "            self.update_gradient(lrate)\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                self.summary(step)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.forward(X)\n",
    "        prediction = []\n",
    "        for i in self.a3:\n",
    "            pr = []\n",
    "            for x in i:\n",
    "                if x >= 0.8:\n",
    "                    pr.append(1)\n",
    "                else:\n",
    "                    pr.append(0)\n",
    "                \n",
    "            prediction.append(pr)\n",
    "        return prediction\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = X_train[:100]\n",
    "y_tr = Y_train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss 108.204634\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 10, Loss 89.320740\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 20, Loss 73.875471\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 30, Loss 61.242679\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 40, Loss 50.910231\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 50, Loss 42.459250\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 60, Loss 35.547134\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 70, Loss 29.893666\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 80, Loss 25.269656\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 90, Loss 21.487646\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 100, Loss 18.394313\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 110, Loss 15.864255\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 120, Loss 13.794903\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 130, Loss 12.102366\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 140, Loss 10.718028\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 150, Loss 9.585768\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 160, Loss 8.659685\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 170, Loss 7.902235\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 180, Loss 7.282712\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 190, Loss 6.775999\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 200, Loss 6.361556\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 210, Loss 6.022579\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 220, Loss 5.745328\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 230, Loss 5.518562\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 240, Loss 5.333089\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 250, Loss 5.181390\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 260, Loss 5.057313\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 270, Loss 4.955830\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 280, Loss 4.872827\n",
      "RMSE: 0.9486832980505129\n",
      "MAE: 8.999999999999993\n",
      "Iteration: 290, Loss 4.804938\n",
      "RMSE: 0.948683298050282\n",
      "MAE: 8.999999999997945\n",
      "Iteration: 300, Loss 4.749411\n",
      "RMSE: 0.9486832980272786\n",
      "MAE: 8.99999999979599\n",
      "Iteration: 310, Loss 4.703995\n",
      "RMSE: 0.9486832970083863\n",
      "MAE: 8.999999990923284\n",
      "Iteration: 320, Loss 4.666848\n",
      "RMSE: 0.9486832737190661\n",
      "MAE: 8.999999789469996\n",
      "Iteration: 330, Loss 4.636460\n",
      "RMSE: 0.9486829610536995\n",
      "MAE: 8.999997097854944\n",
      "Iteration: 340, Loss 4.611557\n",
      "RMSE: 0.9486800687900032\n",
      "MAE: 8.999972177726317\n",
      "Iteration: 350, Loss 4.590718\n",
      "RMSE: 0.9486477490856419\n",
      "MAE: 8.999685711052344\n",
      "Iteration: 360, Loss 2.915828\n",
      "RMSE: 0.4146163503171592\n",
      "MAE: 1.7305177860548684\n",
      "Iteration: 370, Loss 2.183057\n",
      "RMSE: 0.3161711150671918\n",
      "MAE: 1.0099851754843165\n",
      "Iteration: 380, Loss 1.886119\n",
      "RMSE: 0.31623984716755804\n",
      "MAE: 1.0095389183529737\n",
      "Iteration: 390, Loss 1.633766\n",
      "RMSE: 0.3162397602268112\n",
      "MAE: 1.0107382224269008\n",
      "Iteration: 400, Loss 1.427366\n",
      "RMSE: 0.3162397074722568\n",
      "MAE: 1.0132434531246228\n",
      "Iteration: 410, Loss 1.258533\n",
      "RMSE: 0.3162355729264189\n",
      "MAE: 1.018074549583349\n",
      "Iteration: 420, Loss 1.120324\n",
      "RMSE: 0.31620190864755116\n",
      "MAE: 1.0323577913347763\n",
      "Iteration: 430, Loss 1.003968\n",
      "RMSE: 0.31509404402538915\n",
      "MAE: 1.0964004010191537\n",
      "Iteration: 440, Loss 0.917183\n",
      "RMSE: 0.31615139288604216\n",
      "MAE: 1.0117860030172268\n",
      "Iteration: 450, Loss 0.944142\n",
      "RMSE: 0.3093550505062165\n",
      "MAE: 1.0644067170800071\n",
      "Iteration: 460, Loss 0.881883\n",
      "RMSE: 0.31621796735528784\n",
      "MAE: 1.0184311749522017\n",
      "Iteration: 470, Loss 0.816249\n",
      "RMSE: 0.31620119788387635\n",
      "MAE: 1.0139743775312289\n",
      "Iteration: 480, Loss 0.810229\n",
      "RMSE: 0.316220864714463\n",
      "MAE: 1.0042399156931845\n",
      "Iteration: 490, Loss 0.753683\n",
      "RMSE: 0.3162104925205019\n",
      "MAE: 1.0119756320569147\n"
     ]
    }
   ],
   "source": [
    "nn = NNet()\n",
    "nn.train(x_tr,y_tr,500, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss 0.215902\n",
      "RMSE: 0.19879977539652344\n",
      "MAE: 0.5215087210576642\n",
      "Iteration: 10, Loss 0.215816\n",
      "RMSE: 0.19879663882847132\n",
      "MAE: 0.5220631253778258\n",
      "Iteration: 20, Loss 0.215729\n",
      "RMSE: 0.19879311777197717\n",
      "MAE: 0.5226573587970749\n",
      "Iteration: 30, Loss 0.215643\n",
      "RMSE: 0.19878914531547537\n",
      "MAE: 0.5233013909457739\n",
      "Iteration: 40, Loss 0.215556\n",
      "RMSE: 0.1987847336577512\n",
      "MAE: 0.5239795605399005\n",
      "Iteration: 50, Loss 0.215469\n",
      "RMSE: 0.19877966322935242\n",
      "MAE: 0.524733387415442\n",
      "Iteration: 60, Loss 0.215381\n",
      "RMSE: 0.19877390022508953\n",
      "MAE: 0.5255332439591354\n",
      "Iteration: 70, Loss 0.215292\n",
      "RMSE: 0.19876729856187028\n",
      "MAE: 0.5264048156148804\n",
      "Iteration: 80, Loss 0.215201\n",
      "RMSE: 0.198759682219431\n",
      "MAE: 0.5273518426208881\n",
      "Iteration: 90, Loss 0.215107\n",
      "RMSE: 0.19875069615411708\n",
      "MAE: 0.5283880515986558\n",
      "Iteration: 100, Loss 0.215011\n",
      "RMSE: 0.19874010813731469\n",
      "MAE: 0.5295529124338527\n",
      "Iteration: 110, Loss 0.214909\n",
      "RMSE: 0.19872742701153323\n",
      "MAE: 0.5308353139458333\n",
      "Iteration: 120, Loss 0.214802\n",
      "RMSE: 0.1987119949092507\n",
      "MAE: 0.5322895316558918\n",
      "Iteration: 130, Loss 0.214687\n",
      "RMSE: 0.19869282208260347\n",
      "MAE: 0.533948518091534\n",
      "Iteration: 140, Loss 0.214560\n",
      "RMSE: 0.1986685255345013\n",
      "MAE: 0.5358739190393925\n",
      "Iteration: 150, Loss 0.214417\n",
      "RMSE: 0.19863681562565652\n",
      "MAE: 0.5381467871286176\n",
      "Iteration: 160, Loss 0.214248\n",
      "RMSE: 0.19859391799158918\n",
      "MAE: 0.5409461422468641\n",
      "Iteration: 170, Loss 0.214041\n",
      "RMSE: 0.19853313987555826\n",
      "MAE: 0.5444410700457689\n",
      "Iteration: 180, Loss 0.213768\n",
      "RMSE: 0.1984417424095784\n",
      "MAE: 0.5490924351746431\n",
      "Iteration: 190, Loss 0.213376\n",
      "RMSE: 0.19829302799292062\n",
      "MAE: 0.5557113241475995\n",
      "Iteration: 200, Loss 0.212742\n",
      "RMSE: 0.19802664027752742\n",
      "MAE: 0.5659495412266958\n",
      "Iteration: 210, Loss 0.211615\n",
      "RMSE: 0.19751165728305248\n",
      "MAE: 0.5829469647531468\n",
      "Iteration: 220, Loss 0.209696\n",
      "RMSE: 0.1965822389542699\n",
      "MAE: 0.6071352527884989\n",
      "Iteration: 230, Loss 0.206703\n",
      "RMSE: 0.19506545598409625\n",
      "MAE: 0.6252086879726089\n",
      "Iteration: 240, Loss 0.200610\n",
      "RMSE: 0.19187477102817654\n",
      "MAE: 0.6307955652944888\n",
      "Iteration: 250, Loss 0.188500\n",
      "RMSE: 0.18531153390439664\n",
      "MAE: 0.6046003219108822\n",
      "Iteration: 260, Loss 0.178028\n",
      "RMSE: 0.17937320082075298\n",
      "MAE: 0.558064530837722\n",
      "Iteration: 270, Loss 0.172847\n",
      "RMSE: 0.17629795439168702\n",
      "MAE: 0.5268095662530871\n",
      "Iteration: 280, Loss 0.170268\n",
      "RMSE: 0.17471071504004593\n",
      "MAE: 0.5086831534166841\n",
      "Iteration: 290, Loss 0.168827\n",
      "RMSE: 0.17380099559348708\n",
      "MAE: 0.49839322870677466\n",
      "Iteration: 300, Loss 0.167886\n",
      "RMSE: 0.17320371680244037\n",
      "MAE: 0.49313845313575266\n",
      "Iteration: 310, Loss 0.167142\n",
      "RMSE: 0.17274025630939144\n",
      "MAE: 0.49197258586747084\n",
      "Iteration: 320, Loss 0.166358\n",
      "RMSE: 0.1722704518669137\n",
      "MAE: 0.49558868132963846\n",
      "Iteration: 330, Loss 0.165027\n",
      "RMSE: 0.1714963627342942\n",
      "MAE: 0.5085414503618455\n",
      "Iteration: 340, Loss 0.160032\n",
      "RMSE: 0.16855902005886686\n",
      "MAE: 0.5549097150377875\n",
      "Iteration: 350, Loss 0.133174\n",
      "RMSE: 0.1515892534329363\n",
      "MAE: 0.5684820033524279\n",
      "Iteration: 360, Loss 0.111978\n",
      "RMSE: 0.13651980781651962\n",
      "MAE: 0.48601615294091133\n",
      "Iteration: 370, Loss 0.105504\n",
      "RMSE: 0.13143279336704028\n",
      "MAE: 0.4671710874489346\n",
      "Iteration: 380, Loss 0.100814\n",
      "RMSE: 0.1275966016632091\n",
      "MAE: 0.47165525984837986\n",
      "Iteration: 390, Loss 0.091646\n",
      "RMSE: 0.11990060587887441\n",
      "MAE: 0.4737803453940322\n",
      "Iteration: 400, Loss 0.073743\n",
      "RMSE: 0.10334478050230063\n",
      "MAE: 0.41796234935653503\n",
      "Iteration: 410, Loss 0.062909\n",
      "RMSE: 0.09168066211494703\n",
      "MAE: 0.3539147388644456\n",
      "Iteration: 420, Loss 0.058184\n",
      "RMSE: 0.08591430493740132\n",
      "MAE: 0.3161716862247645\n",
      "Iteration: 430, Loss 0.055786\n",
      "RMSE: 0.08272676729045533\n",
      "MAE: 0.2932467429616538\n",
      "Iteration: 440, Loss 0.054412\n",
      "RMSE: 0.0807857421305603\n",
      "MAE: 0.27819603225090844\n",
      "Iteration: 450, Loss 0.053556\n",
      "RMSE: 0.07951990493145449\n",
      "MAE: 0.26786062204879785\n",
      "Iteration: 460, Loss 0.052983\n",
      "RMSE: 0.0786475513244052\n",
      "MAE: 0.2604379981702232\n",
      "Iteration: 470, Loss 0.052575\n",
      "RMSE: 0.07801679403495056\n",
      "MAE: 0.2550528133093358\n",
      "Iteration: 480, Loss 0.052264\n",
      "RMSE: 0.07753955867717009\n",
      "MAE: 0.2511327772334495\n",
      "Iteration: 490, Loss 0.052013\n",
      "RMSE: 0.07716240877864634\n",
      "MAE: 0.24835641715832435\n",
      "Iteration: 500, Loss 0.051796\n",
      "RMSE: 0.07685001938363246\n",
      "MAE: 0.24652491778659816\n",
      "Iteration: 510, Loss 0.051596\n",
      "RMSE: 0.07657677810355656\n",
      "MAE: 0.24555106247016809\n",
      "Iteration: 520, Loss 0.051398\n",
      "RMSE: 0.07631998808598107\n",
      "MAE: 0.24540901124776687\n",
      "Iteration: 530, Loss 0.051185\n",
      "RMSE: 0.0760559673923147\n",
      "MAE: 0.24620266913986263\n",
      "Iteration: 540, Loss 0.050935\n",
      "RMSE: 0.07575540994182892\n",
      "MAE: 0.24823799860982348\n",
      "Iteration: 550, Loss 0.050621\n",
      "RMSE: 0.07537834890413019\n",
      "MAE: 0.25204805280031434\n",
      "Iteration: 560, Loss 0.050216\n",
      "RMSE: 0.07488396644238285\n",
      "MAE: 0.25835846484371333\n",
      "Iteration: 570, Loss 0.049737\n",
      "RMSE: 0.07427993250722628\n",
      "MAE: 0.26726473595557937\n",
      "Iteration: 580, Loss 0.049260\n",
      "RMSE: 0.07365111561232884\n",
      "MAE: 0.27606422572255046\n",
      "Iteration: 590, Loss 0.048802\n",
      "RMSE: 0.07301532899048246\n",
      "MAE: 0.28148161538308647\n",
      "Iteration: 600, Loss 0.048318\n",
      "RMSE: 0.07232015138841787\n",
      "MAE: 0.2842419257159292\n",
      "Iteration: 610, Loss 0.047766\n",
      "RMSE: 0.0715051572104654\n",
      "MAE: 0.28590443084617845\n",
      "Iteration: 620, Loss 0.047079\n",
      "RMSE: 0.07047084526606567\n",
      "MAE: 0.28694087492718334\n",
      "Iteration: 630, Loss 0.046158\n",
      "RMSE: 0.069055813141953\n",
      "MAE: 0.28711430531236887\n",
      "Iteration: 640, Loss 0.044846\n",
      "RMSE: 0.06699358367793674\n",
      "MAE: 0.2857510370613723\n",
      "Iteration: 650, Loss 0.042953\n",
      "RMSE: 0.0639079661779139\n",
      "MAE: 0.2814359343195547\n",
      "Iteration: 660, Loss 0.040574\n",
      "RMSE: 0.05979886314345373\n",
      "MAE: 0.27223852425380024\n",
      "Iteration: 670, Loss 0.038247\n",
      "RMSE: 0.05544542657683967\n",
      "MAE: 0.2594453423016346\n",
      "Iteration: 680, Loss 0.036491\n",
      "RMSE: 0.05185932445355185\n",
      "MAE: 0.2467427701013333\n",
      "Iteration: 690, Loss 0.035325\n",
      "RMSE: 0.049276940251425275\n",
      "MAE: 0.2365635866320245\n",
      "Iteration: 700, Loss 0.034560\n",
      "RMSE: 0.047469055595679537\n",
      "MAE: 0.22910803125211324\n",
      "Iteration: 710, Loss 0.034030\n",
      "RMSE: 0.046159435282315274\n",
      "MAE: 0.22393976442561517\n",
      "Iteration: 720, Loss 0.033624\n",
      "RMSE: 0.0451351619494771\n",
      "MAE: 0.22049479952805273\n",
      "Iteration: 730, Loss 0.033252\n",
      "RMSE: 0.04419749295484117\n",
      "MAE: 0.21872293184642275\n",
      "Iteration: 740, Loss 0.032781\n",
      "RMSE: 0.04303496791546072\n",
      "MAE: 0.21872225575700432\n",
      "Iteration: 750, Loss 0.031926\n",
      "RMSE: 0.04089999335464033\n",
      "MAE: 0.22041490505201236\n",
      "Iteration: 760, Loss 0.030714\n",
      "RMSE: 0.03764315702474873\n",
      "MAE: 0.2189148053745511\n",
      "Iteration: 770, Loss 0.029892\n",
      "RMSE: 0.03517602348153732\n",
      "MAE: 0.21365182775658206\n",
      "Iteration: 780, Loss 0.029437\n",
      "RMSE: 0.033667500943815605\n",
      "MAE: 0.20906359125728508\n",
      "Iteration: 790, Loss 0.029158\n",
      "RMSE: 0.0326726901107758\n",
      "MAE: 0.20563227898376504\n",
      "Iteration: 800, Loss 0.028969\n",
      "RMSE: 0.031968612390579555\n",
      "MAE: 0.20302042852367666\n",
      "Iteration: 810, Loss 0.028830\n",
      "RMSE: 0.03144161751478196\n",
      "MAE: 0.20107746056475875\n",
      "Iteration: 820, Loss 0.028724\n",
      "RMSE: 0.03103557979676144\n",
      "MAE: 0.19954987506278343\n",
      "Iteration: 830, Loss 0.028639\n",
      "RMSE: 0.03071525598317896\n",
      "MAE: 0.1983587069871967\n",
      "Iteration: 840, Loss 0.028571\n",
      "RMSE: 0.03045948081510556\n",
      "MAE: 0.19741838238317602\n",
      "Iteration: 850, Loss 0.028513\n",
      "RMSE: 0.030251618215631805\n",
      "MAE: 0.1966919708337523\n",
      "Iteration: 860, Loss 0.028463\n",
      "RMSE: 0.030081384860990726\n",
      "MAE: 0.1961370992759276\n",
      "Iteration: 870, Loss 0.028420\n",
      "RMSE: 0.029940597241545224\n",
      "MAE: 0.1956992208509822\n",
      "Iteration: 880, Loss 0.028383\n",
      "RMSE: 0.029823300492933708\n",
      "MAE: 0.19536009138747817\n",
      "Iteration: 890, Loss 0.028349\n",
      "RMSE: 0.029725320615275\n",
      "MAE: 0.19511400786867183\n",
      "Iteration: 900, Loss 0.028318\n",
      "RMSE: 0.029642907511402142\n",
      "MAE: 0.19492273270905522\n",
      "Iteration: 910, Loss 0.028291\n",
      "RMSE: 0.02957321733150084\n",
      "MAE: 0.19478408612187065\n",
      "Iteration: 920, Loss 0.028265\n",
      "RMSE: 0.02951420863174894\n",
      "MAE: 0.19469896900560946\n",
      "Iteration: 930, Loss 0.028242\n",
      "RMSE: 0.0294638557543527\n",
      "MAE: 0.19465042256442774\n",
      "Iteration: 940, Loss 0.028220\n",
      "RMSE: 0.029421034799411102\n",
      "MAE: 0.19463176986529615\n",
      "Iteration: 950, Loss 0.028200\n",
      "RMSE: 0.02938417786616092\n",
      "MAE: 0.19462683306855072\n",
      "Iteration: 960, Loss 0.028181\n",
      "RMSE: 0.029352709943253442\n",
      "MAE: 0.1946510403515501\n",
      "Iteration: 970, Loss 0.028163\n",
      "RMSE: 0.029325537964732258\n",
      "MAE: 0.1946884971900435\n",
      "Iteration: 980, Loss 0.028146\n",
      "RMSE: 0.029302077062572353\n",
      "MAE: 0.1947376285534941\n",
      "Iteration: 990, Loss 0.028130\n",
      "RMSE: 0.029281730449851575\n",
      "MAE: 0.19480525221928757\n",
      "Iteration: 1000, Loss 0.028114\n",
      "RMSE: 0.0292641619531323\n",
      "MAE: 0.19488089354398208\n",
      "Iteration: 1010, Loss 0.028100\n",
      "RMSE: 0.02924882784165837\n",
      "MAE: 0.19495916706116342\n",
      "Iteration: 1020, Loss 0.028086\n",
      "RMSE: 0.029235516128908415\n",
      "MAE: 0.19504335255856578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1030, Loss 0.028073\n",
      "RMSE: 0.029223888591797\n",
      "MAE: 0.19512463732637553\n",
      "Iteration: 1040, Loss 0.028060\n",
      "RMSE: 0.029213587956880206\n",
      "MAE: 0.19521656907586912\n",
      "Iteration: 1050, Loss 0.028048\n",
      "RMSE: 0.029204620734343174\n",
      "MAE: 0.19530183026915943\n",
      "Iteration: 1060, Loss 0.028036\n",
      "RMSE: 0.02919677667352374\n",
      "MAE: 0.19538974410635263\n",
      "Iteration: 1070, Loss 0.028025\n",
      "RMSE: 0.02918975889213959\n",
      "MAE: 0.19547291799441296\n",
      "Iteration: 1080, Loss 0.028014\n",
      "RMSE: 0.029183543518312236\n",
      "MAE: 0.19556928249027272\n",
      "Iteration: 1090, Loss 0.028003\n",
      "RMSE: 0.02917799334313986\n",
      "MAE: 0.19565878839312562\n",
      "Iteration: 1100, Loss 0.027993\n",
      "RMSE: 0.029172940425261702\n",
      "MAE: 0.19573970824971357\n",
      "Iteration: 1110, Loss 0.027983\n",
      "RMSE: 0.02916839033944073\n",
      "MAE: 0.19583305868800083\n",
      "Iteration: 1120, Loss 0.027974\n",
      "RMSE: 0.029164189395063298\n",
      "MAE: 0.1959230605938832\n",
      "Iteration: 1130, Loss 0.027965\n",
      "RMSE: 0.02916049585145942\n",
      "MAE: 0.19600409004193822\n",
      "Iteration: 1140, Loss 0.027956\n",
      "RMSE: 0.029156996490824948\n",
      "MAE: 0.19608743978640292\n",
      "Iteration: 1150, Loss 0.027947\n",
      "RMSE: 0.029153722988436374\n",
      "MAE: 0.1961700227294719\n",
      "Iteration: 1160, Loss 0.027939\n",
      "RMSE: 0.02915069514009959\n",
      "MAE: 0.1962439252668741\n",
      "Iteration: 1170, Loss 0.027931\n",
      "RMSE: 0.02914777486090547\n",
      "MAE: 0.1963252397473392\n",
      "Iteration: 1180, Loss 0.027923\n",
      "RMSE: 0.029145117246465516\n",
      "MAE: 0.1963930514624744\n",
      "Iteration: 1190, Loss 0.027915\n",
      "RMSE: 0.029142499598576607\n",
      "MAE: 0.19647076880409298\n",
      "Iteration: 1200, Loss 0.027907\n",
      "RMSE: 0.02914015031835911\n",
      "MAE: 0.19653992879282256\n",
      "Iteration: 1210, Loss 0.027900\n",
      "RMSE: 0.029137798262556216\n",
      "MAE: 0.19660684511759116\n",
      "Iteration: 1220, Loss 0.027893\n",
      "RMSE: 0.02913552526379502\n",
      "MAE: 0.19667715837217314\n",
      "Iteration: 1230, Loss 0.027886\n",
      "RMSE: 0.02913342757289801\n",
      "MAE: 0.19673753773040326\n",
      "Iteration: 1240, Loss 0.027879\n",
      "RMSE: 0.02913135952085045\n",
      "MAE: 0.19680349835338248\n",
      "Iteration: 1250, Loss 0.027873\n",
      "RMSE: 0.02912921561194213\n",
      "MAE: 0.1968690212030782\n",
      "Iteration: 1260, Loss 0.027866\n",
      "RMSE: 0.02912722245252967\n",
      "MAE: 0.19693548672152478\n",
      "Iteration: 1270, Loss 0.027860\n",
      "RMSE: 0.029125246433639864\n",
      "MAE: 0.19699179763279331\n",
      "Iteration: 1280, Loss 0.027854\n",
      "RMSE: 0.029123295032196655\n",
      "MAE: 0.1970460201278843\n",
      "Iteration: 1290, Loss 0.027848\n",
      "RMSE: 0.029121526786543867\n",
      "MAE: 0.19710141778020618\n",
      "Iteration: 1300, Loss 0.027842\n",
      "RMSE: 0.02911954268499223\n",
      "MAE: 0.19716184318987245\n",
      "Iteration: 1310, Loss 0.027836\n",
      "RMSE: 0.02911769651184167\n",
      "MAE: 0.1972166570891279\n",
      "Iteration: 1320, Loss 0.027830\n",
      "RMSE: 0.029115828141061444\n",
      "MAE: 0.1972712471221021\n",
      "Iteration: 1330, Loss 0.027825\n",
      "RMSE: 0.02911402702761826\n",
      "MAE: 0.1973145343220169\n",
      "Iteration: 1340, Loss 0.027819\n",
      "RMSE: 0.029112204324523158\n",
      "MAE: 0.19736182342044026\n",
      "Iteration: 1350, Loss 0.027814\n",
      "RMSE: 0.02911034871541536\n",
      "MAE: 0.1974147444248809\n",
      "Iteration: 1360, Loss 0.027809\n",
      "RMSE: 0.029108553024985738\n",
      "MAE: 0.1974639792125518\n",
      "Iteration: 1370, Loss 0.027804\n",
      "RMSE: 0.0291067504269562\n",
      "MAE: 0.19750213447946602\n",
      "Iteration: 1380, Loss 0.027799\n",
      "RMSE: 0.02910488024629647\n",
      "MAE: 0.19755355943658892\n",
      "Iteration: 1390, Loss 0.027794\n",
      "RMSE: 0.02910313902117489\n",
      "MAE: 0.19759423664539524\n",
      "Iteration: 1400, Loss 0.027789\n",
      "RMSE: 0.029101393002704676\n",
      "MAE: 0.19763524399464438\n",
      "Iteration: 1410, Loss 0.027785\n",
      "RMSE: 0.02909967703305003\n",
      "MAE: 0.1976756286478436\n",
      "Iteration: 1420, Loss 0.027780\n",
      "RMSE: 0.029097853114709075\n",
      "MAE: 0.19771805363072215\n",
      "Iteration: 1430, Loss 0.027776\n",
      "RMSE: 0.02909612892412653\n",
      "MAE: 0.197761770092796\n",
      "Iteration: 1440, Loss 0.027771\n",
      "RMSE: 0.029094408399192806\n",
      "MAE: 0.197796025206773\n",
      "Iteration: 1450, Loss 0.027767\n",
      "RMSE: 0.02909255727251054\n",
      "MAE: 0.19784486446906066\n",
      "Iteration: 1460, Loss 0.027762\n",
      "RMSE: 0.029090951441198533\n",
      "MAE: 0.1978734367468538\n",
      "Iteration: 1470, Loss 0.027758\n",
      "RMSE: 0.029089263431279613\n",
      "MAE: 0.19790961298138768\n",
      "Iteration: 1480, Loss 0.027754\n",
      "RMSE: 0.02908751957028736\n",
      "MAE: 0.19795147857565573\n",
      "Iteration: 1490, Loss 0.027750\n",
      "RMSE: 0.029085896142964834\n",
      "MAE: 0.1979845387302336\n",
      "Iteration: 1500, Loss 0.027746\n",
      "RMSE: 0.02908418203479084\n",
      "MAE: 0.198018433455493\n",
      "Iteration: 1510, Loss 0.027742\n",
      "RMSE: 0.029082469258436675\n",
      "MAE: 0.19805259417825674\n",
      "Iteration: 1520, Loss 0.027739\n",
      "RMSE: 0.02908086297648931\n",
      "MAE: 0.19808986969970696\n",
      "Iteration: 1530, Loss 0.027735\n",
      "RMSE: 0.029079172872396985\n",
      "MAE: 0.19812120693768875\n",
      "Iteration: 1540, Loss 0.027731\n",
      "RMSE: 0.029077564245387252\n",
      "MAE: 0.19815264164222557\n",
      "Iteration: 1550, Loss 0.027727\n",
      "RMSE: 0.029075934548424092\n",
      "MAE: 0.1981743926131567\n",
      "Iteration: 1560, Loss 0.027724\n",
      "RMSE: 0.029074333001422484\n",
      "MAE: 0.19820743004055935\n",
      "Iteration: 1570, Loss 0.027720\n",
      "RMSE: 0.029072756308257928\n",
      "MAE: 0.19823606336346727\n",
      "Iteration: 1580, Loss 0.027717\n",
      "RMSE: 0.029071196438922874\n",
      "MAE: 0.1982764220758457\n",
      "Iteration: 1590, Loss 0.027714\n",
      "RMSE: 0.029069651231786293\n",
      "MAE: 0.19829998591747405\n",
      "Iteration: 1600, Loss 0.027710\n",
      "RMSE: 0.029068049754366644\n",
      "MAE: 0.19833818726998445\n",
      "Iteration: 1610, Loss 0.027707\n",
      "RMSE: 0.029066524493276232\n",
      "MAE: 0.19836619929579108\n",
      "Iteration: 1620, Loss 0.027704\n",
      "RMSE: 0.02906505721450664\n",
      "MAE: 0.19838367120366576\n",
      "Iteration: 1630, Loss 0.027701\n",
      "RMSE: 0.029063558766072875\n",
      "MAE: 0.1984119068298527\n",
      "Iteration: 1640, Loss 0.027697\n",
      "RMSE: 0.029061969443644503\n",
      "MAE: 0.1984449030799342\n",
      "Iteration: 1650, Loss 0.027694\n",
      "RMSE: 0.029060537751664894\n",
      "MAE: 0.19847611519351274\n",
      "Iteration: 1660, Loss 0.027691\n",
      "RMSE: 0.029059079854146286\n",
      "MAE: 0.19849184788645197\n",
      "Iteration: 1670, Loss 0.027688\n",
      "RMSE: 0.02905759984334781\n",
      "MAE: 0.19851909561766015\n",
      "Iteration: 1680, Loss 0.027686\n",
      "RMSE: 0.029056093598267475\n",
      "MAE: 0.19854576503469407\n",
      "Iteration: 1690, Loss 0.027683\n",
      "RMSE: 0.029054660832028564\n",
      "MAE: 0.19856841354275975\n",
      "Iteration: 1700, Loss 0.027680\n",
      "RMSE: 0.029053229901612004\n",
      "MAE: 0.1986024993658578\n",
      "Iteration: 1710, Loss 0.027677\n",
      "RMSE: 0.02905188035953529\n",
      "MAE: 0.1986192523732721\n",
      "Iteration: 1720, Loss 0.027674\n",
      "RMSE: 0.02905043741989038\n",
      "MAE: 0.19864307618696536\n",
      "Iteration: 1730, Loss 0.027672\n",
      "RMSE: 0.02904896834574367\n",
      "MAE: 0.1986718654497583\n",
      "Iteration: 1740, Loss 0.027669\n",
      "RMSE: 0.02904767453221809\n",
      "MAE: 0.1986872850129683\n",
      "Iteration: 1750, Loss 0.027666\n",
      "RMSE: 0.029046344591909738\n",
      "MAE: 0.19871848480890242\n",
      "Iteration: 1760, Loss 0.027664\n",
      "RMSE: 0.029045000096632252\n",
      "MAE: 0.19873670323317635\n",
      "Iteration: 1770, Loss 0.027661\n",
      "RMSE: 0.02904355635602339\n",
      "MAE: 0.19876479811417044\n",
      "Iteration: 1780, Loss 0.027659\n",
      "RMSE: 0.029042267486463894\n",
      "MAE: 0.19877908506734485\n",
      "Iteration: 1790, Loss 0.027656\n",
      "RMSE: 0.029040886088363572\n",
      "MAE: 0.19880190878288354\n",
      "Iteration: 1800, Loss 0.027654\n",
      "RMSE: 0.029039576993160048\n",
      "MAE: 0.19882306157370586\n",
      "Iteration: 1810, Loss 0.027652\n",
      "RMSE: 0.02903829308872717\n",
      "MAE: 0.19884911842566708\n",
      "Iteration: 1820, Loss 0.027649\n",
      "RMSE: 0.029036993362581743\n",
      "MAE: 0.19886782676816503\n",
      "Iteration: 1830, Loss 0.027647\n",
      "RMSE: 0.02903562232084504\n",
      "MAE: 0.198885918484552\n",
      "Iteration: 1840, Loss 0.027645\n",
      "RMSE: 0.029034336544883813\n",
      "MAE: 0.19891216433833145\n",
      "Iteration: 1850, Loss 0.027642\n",
      "RMSE: 0.02903304563443577\n",
      "MAE: 0.19891504233611695\n",
      "Iteration: 1860, Loss 0.027640\n",
      "RMSE: 0.02903175202202977\n",
      "MAE: 0.19893829382912093\n",
      "Iteration: 1870, Loss 0.027638\n",
      "RMSE: 0.02903042319291429\n",
      "MAE: 0.19896196706293176\n",
      "Iteration: 1880, Loss 0.027636\n",
      "RMSE: 0.029029170075430853\n",
      "MAE: 0.19899147474348988\n",
      "Iteration: 1890, Loss 0.027634\n",
      "RMSE: 0.029027893174710693\n",
      "MAE: 0.19899953420735683\n",
      "Iteration: 1900, Loss 0.027631\n",
      "RMSE: 0.02902667132997411\n",
      "MAE: 0.19902402542855627\n",
      "Iteration: 1910, Loss 0.027629\n",
      "RMSE: 0.029025524346776924\n",
      "MAE: 0.19903127069162213\n",
      "Iteration: 1920, Loss 0.027627\n",
      "RMSE: 0.029024183228160364\n",
      "MAE: 0.1990604290726096\n",
      "Iteration: 1930, Loss 0.027625\n",
      "RMSE: 0.02902299458441203\n",
      "MAE: 0.19907408819023614\n",
      "Iteration: 1940, Loss 0.027623\n",
      "RMSE: 0.029021891980811773\n",
      "MAE: 0.19908535958585705\n",
      "Iteration: 1950, Loss 0.027621\n",
      "RMSE: 0.029020593985471178\n",
      "MAE: 0.19911054033492812\n",
      "Iteration: 1960, Loss 0.027619\n",
      "RMSE: 0.02901954604510463\n",
      "MAE: 0.1991187831953771\n",
      "Iteration: 1970, Loss 0.027617\n",
      "RMSE: 0.029018283742611253\n",
      "MAE: 0.19914993725650498\n",
      "Iteration: 1980, Loss 0.027616\n",
      "RMSE: 0.02901716066596323\n",
      "MAE: 0.19915677618208127\n",
      "Iteration: 1990, Loss 0.027614\n",
      "RMSE: 0.029016018382840656\n",
      "MAE: 0.19917220868912597\n"
     ]
    }
   ],
   "source": [
    "nn.train(x_tr,y_tr,5000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pr = nn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]],\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pr[:5], Y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss 90.383741\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 10, Loss 14.941458\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n"
     ]
    }
   ],
   "source": [
    "nn = NNet()\n",
    "nn.train(X_train,Y_train,20, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss 6.036019\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 10, Loss 5.756321\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 20, Loss 5.527554\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 30, Loss 5.340443\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n",
      "Iteration: 40, Loss 5.187404\n",
      "RMSE: 0.9486832980505138\n",
      "MAE: 9.0\n"
     ]
    }
   ],
   "source": [
    "nn.train(X_train,Y_train,50, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss 4.663924\n",
      "RMSE: 0.9486828936359367\n",
      "MAE: 8.999996496774859\n",
      "Iteration: 10, Loss 4.609201\n",
      "RMSE: 0.948670103696047\n",
      "MAE: 8.999884823209706\n",
      "Iteration: 20, Loss 3.259583\n",
      "RMSE: 0.42113273778183513\n",
      "MAE: 1.7779084297460501\n",
      "Iteration: 30, Loss 2.472482\n",
      "RMSE: 0.4213240999692487\n",
      "MAE: 1.7847787075300388\n",
      "Iteration: 40, Loss 1.978943\n",
      "RMSE: 0.42133925838532105\n",
      "MAE: 1.776024464075981\n",
      "Iteration: 50, Loss 1.616166\n",
      "RMSE: 0.4213350674799915\n",
      "MAE: 1.7795272542602838\n",
      "Iteration: 60, Loss 1.372648\n",
      "RMSE: 0.42109013071093937\n",
      "MAE: 1.830105441031316\n",
      "Iteration: 70, Loss 1.753654\n",
      "RMSE: 0.3162275134711301\n",
      "MAE: 1.0000314805418522\n",
      "Iteration: 80, Loss 1.336903\n",
      "RMSE: 0.316219635937028\n",
      "MAE: 1.0005121452060606\n",
      "Iteration: 90, Loss 1.294495\n",
      "RMSE: 0.31622776601682256\n",
      "MAE: 1.000000000008215\n"
     ]
    }
   ],
   "source": [
    "nn.train(X_train,Y_train,100, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss 0.831219\n",
      "RMSE: 0.31622775218013266\n",
      "MAE: 1.000001189074866\n",
      "Iteration: 10, Loss 0.799624\n",
      "RMSE: 0.3162277274672914\n",
      "MAE: 1.0000030278384675\n",
      "Iteration: 20, Loss 0.771042\n",
      "RMSE: 0.31622766491181825\n",
      "MAE: 1.000007331854353\n",
      "Iteration: 30, Loss 0.745187\n",
      "RMSE: 0.31622751487109363\n",
      "MAE: 1.0000169442165527\n",
      "Iteration: 40, Loss 0.721796\n",
      "RMSE: 0.3162271720713753\n",
      "MAE: 1.0000374932963332\n",
      "Iteration: 50, Loss 0.700634\n",
      "RMSE: 0.31622642193124856\n",
      "MAE: 1.0000797010365867\n",
      "Iteration: 60, Loss 0.681486\n",
      "RMSE: 0.3162248381970556\n",
      "MAE: 1.0001634442971135\n",
      "Iteration: 70, Loss 0.664153\n",
      "RMSE: 0.3162215742128015\n",
      "MAE: 1.0003253384954596\n",
      "Iteration: 80, Loss 0.648452\n",
      "RMSE: 0.3162148693707178\n",
      "MAE: 1.0006349993381678\n",
      "Iteration: 90, Loss 0.634205\n",
      "RMSE: 0.3162005818970938\n",
      "MAE: 1.0012385265810488\n"
     ]
    }
   ],
   "source": [
    "nn.train(X_train,Y_train,100, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6224614121561942"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.cost_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.45073719e-22, 1.31221416e-37, 9.39743928e-06, 1.50524618e-08,\n",
       "       1.31732502e-08, 1.68237017e-11, 7.01334270e-24, 1.33064159e-05,\n",
       "       1.78255350e-06, 1.69464140e-37])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.a3[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.forward(X_train[100:200])\n",
    "y_pr = nn.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.43343573e-19, 3.30683425e-30, 9.14880020e-05, 4.71513809e-07,\n",
       "       4.22109282e-07, 1.21067342e-09, 6.28311241e-20, 1.34801432e-04,\n",
       "       2.40356142e-05, 1.19734562e-29])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pr[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mix type of y not allowed, got types {'continuous-multioutput', 'multilabel-indicator'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-686f1bba2eb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits)\u001b[0m\n\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1421\u001b[1;33m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1422\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1423\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[1;34m(*ys)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mys_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Mix type of y not allowed, got types %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mys_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0mlabel_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mys_types\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Mix type of y not allowed, got types {'continuous-multioutput', 'multilabel-indicator'}"
     ]
    }
   ],
   "source": [
    "classification_report(Y_test, y_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def set_weights(self, weights):\n",
    "        x = 784 * 800\n",
    "        self.w1 = np.reshape(weights[0:x], (784, 800))\n",
    "        self.w2 = np.reshape(weights[x:x+8000], (800, 10))\n",
    "        \n",
    "           \n",
    "    def check_gradients(self):\n",
    "        self.compute_gradients()\n",
    "        self.compute_numerical_gradients()\n",
    "        print(\"Gradient checked: \" + str(np.linalg.norm(self.gradient - self.numericalGradient) / np.linalg.norm(\n",
    "            self.gradient + self.numericalGradient)))    \n",
    "\n",
    "        \n",
    "    def compute_gradients(self):\n",
    "        nn.forward()\n",
    "        nn.backward()\n",
    "        self.gradient = np.concatenate((self.djdw1.ravel(), self.djdw2.ravel()), axis=1).T\n",
    "\n",
    "        \n",
    "    def compute_numerical_gradients(self):\n",
    "        weights = np.concatenate((self.w1.ravel(), self.w2.ravel()), axis=1).T\n",
    "\n",
    "        self.numericalGradient = np.zeros(weights.shape)\n",
    "        perturbation = np.zeros(weights.shape)\n",
    "        e = 1e-4\n",
    "\n",
    "        for p in range(len(weights)):\n",
    "            # Set perturbation vector\n",
    "            perturbation[p] = e\n",
    "\n",
    "            self.set_weights(weights + perturbation)\n",
    "            self.forward()\n",
    "            loss2 = self.cost_function()\n",
    "\n",
    "            self.set_weights(weights - perturbation)\n",
    "            self.forward()\n",
    "            loss1 = self.cost_function()\n",
    "\n",
    "            self.numericalGradient[p] = (loss2 - loss1) / (2 * e)\n",
    "\n",
    "            perturbation[p] = 0\n",
    "\n",
    "        self.set_weights(weights)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
